accelerate==0.28.0
datasets==2.18.0
einops==0.8.0
torch==2.1.2
flash_attn==2.5.8
numpy==2.1.0
pytest==8.1.1
ring_flash_attn@git+https://github.com/zhuzilin/ring-flash-attention
tiktoken==0.5.2
tqdm==4.65.0
transformers==4.39.1
triton==2.1.0
xformers==0.0.23.post1
deepspeed==0.14.0
yunchang==0.1
